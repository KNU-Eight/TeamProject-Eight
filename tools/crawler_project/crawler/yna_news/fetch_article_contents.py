import requests
from bs4 import BeautifulSoup
import json
import os
from tqdm import tqdm

def extract_article_info(url: str):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "lxml")

        # ì œëª©
        title_tag = soup.select_one("h1.tit01")
        title = title_tag.get_text(strip=True) if title_tag else "[ì œëª© ì—†ìŒ]"

        # ë³¸ë¬¸
        content_paragraphs = soup.select("div.article p, div.story-news p")
        if not content_paragraphs:
            raise ValueError("ë³¸ë¬¸ ì—†ìŒ")

        content = "\n".join(
            p.get_text(strip=True)
            for p in content_paragraphs
            if p.get_text(strip=True)
        )

        if not content.strip():
            print(f"âš ï¸ ë³¸ë¬¸ ì—†ìŒ: {url}")
            return None

        # ë‚ ì§œ
        published_div = soup.select_one("div#newsUpdateTime01")
        published_time = published_div.get("data-published-time") if published_div else "[ë‚ ì§œ ì—†ìŒ]"

        return {
            "url": url,
            "title": title,
            "date": published_time,
            "content": content,
        }

    except Exception as e:
        print(f"âŒ ì‹¤íŒ¨: {url} â€” {e}")
        save_failed_html(url, soup if 'soup' in locals() else "")
        log_failed_url(url)
        return None


def save_failed_html(url: str, soup):
    article_id = url.split("/")[-1].split("?")[0]
    save_path = f"data/failed_html/{article_id}.html"
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    with open(save_path, "w", encoding="utf-8") as f:
        f.write(str(soup))


def log_failed_url(url: str):
    with open("data/failed_urls.txt", "a", encoding="utf-8") as f:
        f.write(url + "\n")


def fetch_article_contents(input_path, output_path):
    if not os.path.exists(input_path):
        print(f"ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_path}")
        return

    with open(input_path, "r", encoding="utf-8") as f:
        urls = json.load(f)

    results = []
    for url in tqdm(urls, desc="ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘"):
        info = extract_article_info(url)
        if info:
            results.append(info)

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“„ ë³¸ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ: {len(results)}ê±´ ì €ì¥ë¨")

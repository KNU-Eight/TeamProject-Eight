# run_crawler.py
import os
import json
import logging
import requests
from tqdm import tqdm

from crawler.list_crawler import crawl_press_list
from crawler.detail_filelink import get_filelink
from crawler.downloader import download_pdf
from extractor.text_extractor import extract_pdf_text
from extractor.paragraph_splitter import make_paragraphs, is_valid_para
from extractor.embedding_service import embed_texts
base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
chunk_path = os.path.join(base_dir, "data", "molit", "chunks.json")
embedding_path = os.path.join(base_dir, "data", "molit", "embedded_chunks.json")
pdf_dir = os.path.join(base_dir, "data", "molit", "pdfs")
KEYWORDS = ["ì „ì„¸ì‚¬ê¸°", "ê¹¡í†µì „ì„¸", "ë³´ì¦ê¸ˆ", "í”¼í•´ì", "ì§€ì›", "ê³µê³µì„ëŒ€", "ì„ëŒ€ì°¨", "ê³„ì•½", "%", "ê³µì‹œ", "ê³µê¸‰"]

os.makedirs(pdf_dir, exist_ok=True)
def run_crawling_and_embedding():
    chunks = []

    print("ğŸ“„ êµ­í† ë¶€ ë³´ë„ìë£Œ í¬ë¡¤ë§ ì‹œì‘")
    for page in range(1, 6):
        articles = crawl_press_list(page=page)
        for article in articles:
            attachments = get_filelink(article['link']) or []
            article["attachments"] = attachments
            for item in attachments:
                if "file_url" in item:
                    try:
                        pdf_path = download_pdf(item["file_url"], article["filename"])
                        lines = extract_pdf_text(pdf_path)
                        paragraphs = make_paragraphs(lines)

                        for para in paragraphs:
                            if is_valid_para(para, KEYWORDS):
                                chunks.append({
                                    "text": para.strip(),
                                    "title": article["title"],
                                    "date": article["date"],
                                    "link": item["file_url"]
                                })

                    except requests.exceptions.RequestException as e:
                        logging.error(f"PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {item['file_url']} - {e}")
                    except Exception as e:
                        logging.error(f"ê¸°íƒ€ ì—ëŸ¬: {e}")

    os.makedirs(os.path.dirname(chunk_path), exist_ok=True)
    with open(chunk_path, "w", encoding="utf-8") as f:
        json.dump(chunks, f, ensure_ascii=False, indent=2)
    print(f"\n ì™„ë£Œ! ì´ {len(chunks)}ê°œ ë¬¸ë‹¨ ì €ì¥ â†’ {chunk_path}")

    texts = [item["text"] for item in chunks if isinstance(item["text"], str) and item["text"].strip()]
    vectors = batch_embed_texts(texts, batch_size=50)

    embedded_chunks = []
    for item, vector in zip(chunks, vectors):
        item_with_embedding = item.copy()
        item_with_embedding["embedding"] = vector
        embedded_chunks.append(item_with_embedding)

    with open(embedding_path, "w", encoding="utf-8") as f:
        json.dump(embedded_chunks, f, ensure_ascii=False, indent=2)
    print(f" ì„ë² ë”© ì €ì¥ ì™„ë£Œ â†’ {embedding_path}")

def batch_embed_texts(texts, batch_size=50):
    from extractor.embedding_service import embed_texts
    all_vectors = []
    print(f"\n ë¬¸ë‹¨ {len(texts)}ê°œ ì„ë² ë”© ì¤‘...")
    for i in tqdm(range(0, len(texts), batch_size), desc="Embedding"):
        batch = texts[i:i + batch_size]
        try:
            vectors = embed_texts(batch)
            all_vectors.extend(vectors)
        except Exception as e:
            logging.error(f" ì„ë² ë”© ì‹¤íŒ¨ (index {i}): {e}")
    return all_vectors

if __name__ == "__main__":
    run_crawling_and_embedding()

# import os
# import json
# import logging
# import requests
# import uvicorn
# from tqdm import tqdm

# from crawler.list_crawler import crawl_press_list
# from crawler.detail_filelink import get_filelink
# from crawler.downloader import download_pdf
# from extractor.text_extractor import extract_pdf_text
# from extractor.paragraph_splitter import make_paragraphs, is_valid_para
# from extractor.embedding_service import embed_texts


# base_dir = os.path.dirname(__file__)
# chunk_path = os.path.join(base_dir, "search", "chunks.json")
# embedding_path = os.path.join(base_dir, "search", "embedded_chunks.json")

# KEYWORDS = ["ì „ì„¸ì‚¬ê¸°", "ê¹¡í†µì „ì„¸", "ë³´ì¦ê¸ˆ", "í”¼í•´ì", "ì§€ì›", "ê³µê³µì„ëŒ€", "ì„ëŒ€ì°¨", "ê³„ì•½", "%", "ê³µì‹œ", "ê³µê¸‰"]

# def run_crawling_and_embedding():
#     chunks = []

#     print("ğŸ“„ êµ­í† ë¶€ ë³´ë„ìë£Œ í¬ë¡¤ë§ ì‹œì‘")
#     for page in range(1, 6):  # 1~5 í˜ì´ì§€ í¬ë¡¤ë§
#         articles = crawl_press_list(page=page)
#         for article in articles:
#             attachments = get_filelink(article['link'])
#             article["attachments"] = attachments
#             for item in attachments:
#                 if "file_url" in item:
#                     try:
#                         pdf_path = download_pdf(item["file_url"], article["filename"])
#                         lines = extract_pdf_text(pdf_path)
#                         paragraphs = make_paragraphs(lines)

#                         for para in paragraphs:
#                             if is_valid_para(para, KEYWORDS):
#                                 chunks.append({
#                                     "text": para.strip(),
#                                     "title": article["title"],
#                                     "date": article["date"],
#                                     "link": item["file_url"]
#                                 })

#                     except requests.exceptions.RequestException as e:
#                         logging.error(f"PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {item['file_url']} - {e}")
#                     except (OSError, IOError) as e:
#                         logging.error(f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {article['title']}.pdf - {e}")
#                     except Exception as e:
#                         logging.error(f"ê¸°íƒ€ ì—ëŸ¬: {e}")

#     os.makedirs(os.path.dirname(chunk_path), exist_ok=True)
#     with open(chunk_path, "w", encoding="utf-8") as f:
#         json.dump(chunks, f, ensure_ascii=False, indent=2)
#     print(f"\nğŸ“¦ ì™„ë£Œ! ì´ {len(chunks)}ê°œ ë¬¸ë‹¨ ì €ì¥ â†’ {chunk_path}")

#     # ì„ë² ë”© ì „ í•„í„°ë§
#     texts = [item["text"] for item in chunks if isinstance(item["text"], str) and item["text"].strip()]
#     vectors = batch_embed_texts(texts, batch_size=50)

#     embedded_chunks = []
#     for item, vector in zip(chunks, vectors):
#         item_with_embedding = item.copy()
#         item_with_embedding["embedding"] = vector
#         embedded_chunks.append(item_with_embedding)

#     with open(embedding_path, "w", encoding="utf-8") as f:
#         json.dump(embedded_chunks, f, ensure_ascii=False, indent=2)
#     print(f"âœ… ì„ë² ë”© ì €ì¥ ì™„ë£Œ â†’ {embedding_path}")

# def batch_embed_texts(texts, batch_size=50):
#     all_vectors = []
#     print(f"\nğŸ”„ ë¬¸ë‹¨ {len(texts)}ê°œ ì„ë² ë”© ì¤‘...")
#     for i in tqdm(range(0, len(texts), batch_size), desc="Embedding"):
#         batch = texts[i:i + batch_size]
#         batch = [t for t in batch if isinstance(t, str) and t.strip()]
#         try:
#             vectors = embed_texts(batch)
#             all_vectors.extend(vectors)
#         except Exception as e:
#             logging.error(f"âŒ ì„ë² ë”© ì‹¤íŒ¨ (index {i}): {e}")
#     return all_vectors

# if __name__ == "__main__":
#     run_crawling_and_embedding()
#     uvicorn.run("crawler.molit.search.search_server:app", host="0.0.0.0", port=8000, reload=True)

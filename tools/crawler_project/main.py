import os
import json
import numpy as np
import uvicorn
import openai
import faiss

from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel
from dotenv import load_dotenv
from fastapi.middleware.cors import CORSMiddleware
from crawler.molit.extractor.embedding_service import embed_texts
from contextlib import asynccontextmanager # Lifespanì„ ìœ„í•´ ì¶”ê°€

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY_DEV"))
    print("OpenAI client initialized.")
except Exception as e:
    print(f"Error initializing OpenAI client: {e}")
    client = None

OPENAI_MODEL = "gpt-4o-mini"

base_dir = os.path.abspath(os.path.dirname(__file__))
cache_dir = os.path.join(base_dir, "cache")
# Lifespan í•¨ìˆ˜ ì •ì˜
@asynccontextmanager
async def lifespan(app: FastAPI):
    print("ğŸ”„ Loading FAISS index and reference data...")
    try:
        faiss_index = faiss.read_index(os.path.join(cache_dir, "faiss.index"))
        with open(os.path.join(cache_dir, "combined_data.json"), encoding="utf-8") as f:
            reference_data = json.load(f)

        app.state.faiss_index = faiss_index
        app.state.reference_data = reference_data
        print(f"Loaded FAISS index ({faiss_index.ntotal} vectors) and reference data.")

    except Exception as e:
        print(f" Failed to load search data: {e}")
        app.state.faiss_index = None
        app.state.reference_data = []

    yield
    print("Server shutdown: Cleaning up resources...")

# FastAPI ì•± ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œ lifespan ì „ë‹¬
app = FastAPI(lifespan=lifespan)

# CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
origins = [
    "http://localhost",
    "http://localhost:8000",
    "http://10.0.2.2:8000",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic ëª¨ë¸ ì •ì˜ ---
class PromptRequest(BaseModel):
    prompt: str

class LLMResponse(BaseModel):
    response: str

class SearchRequest(BaseModel):
    query: str
    top_k: int = 3
    
    
@app.post("/chat", response_model=LLMResponse)
async def chat_with_llm(request: PromptRequest, http_request: Request):  # â† Request ê°ì²´ ì¶”ê°€
    if client is None:
        raise HTTPException(status_code=500, detail="OpenAI client is not initialized.")

    try:
        print(f"Received prompt for /chat: {request.prompt}")

        # --- 1. FAISS ê²€ìƒ‰ ë¡œì§ í¬í•¨ ---
        index = app.state.faiss_index 
        search_data_list = app.state.reference_data

        if index is None or not search_data_list:
            raise HTTPException(status_code=503, detail="Search index not available.")

        query_vec = embed_texts([request.prompt])[0]
        query_vec = np.array([query_vec], dtype="float32")
        faiss.normalize_L2(query_vec)

        top_k = 10 # ê²€ìƒ‰í•  ë¬¸ë‹¨ ìˆ˜
        distances, indices = index.search(query_vec, top_k)

        context_chunks = []
        for i in indices[0]:
            if i != -1:
                item = search_data_list[i]
                context_chunks.append(item["text"])

        if not context_chunks:
            raise HTTPException(status_code=404, detail="No relevant context found.")

        context_text = "\n\n".join(f"- {chunk}" for chunk in context_chunks)

        messages = [
            {
                "role": "system",
                "content": (
                            "ë‹¹ì‹ ì€ ì „ì„¸ì‚¬ê¸° ë° ì„ëŒ€ ì •ì±…ì— ëŒ€í•´ ì‹ ë¢°ì„± ìˆê³  ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤. "
            "ë‹¤ìŒì€ ì°¸ê³  ë¬¸ë‹¨ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ì´ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í•´ìš”ì²´ë¡œ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n\n"
            "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì´ëª¨í‹°ì½˜ë„ ì ì ˆíˆ ì‚¬ìš©í•˜ë©´ì„œ ë”°ëœ»í•˜ê³  ì¹œì ˆí•œ ë§íˆ¬ë¡œ í•´ì£¼ì„¸ìš”. "
            "ì „ì„¸ ì„ëŒ€ì— ëŒ€í•´ ì˜ ëª¨ë¥´ëŠ” ì‚¬íšŒì´ˆë…„ìƒì´ë¼ê³  ìƒê°í•˜ê³  ë‹µë³€ì„ í•´ì£¼ì„¸ìš”. ì–´ë ¤ìš´ ìš©ì–´ì—ì„œëŠ” ì‰½ê²Œ ì„¤ëª…ì„ ë§ë¶™ì—¬ ì£¼ì„¸ìš”.\n\n"
            f"{context_text}"
                )
            },
            {
                "role": "user",
                "content": request.prompt
            }
        ]
        
        # --- 3. LLM í˜¸ì¶œ ---
        completion = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=messages
        )

        llm_response_text = completion.choices[0].message.content
        return LLMResponse(response=llm_response_text)

    except openai.AuthenticationError as e:
        raise HTTPException(status_code=401, detail=f"OpenAI Authentication Error: {e}")
    except openai.APIError as e:
        raise HTTPException(status_code=500, detail=f"OpenAI API Error: {e}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An internal server error occurred: {e}")

# @app.post("/search")
# def search(req: SearchRequest, request: Request):
#     index = request.app.state.faiss_index
#     search_data_list = request.app.state.reference_data  

#     if index is None or not search_data_list:
#         raise HTTPException(status_code=503, detail="Search index not available.")

#     query_vec = embed_texts([req.query])[0]
#     query_vec = np.array([query_vec], dtype="float32")
#     faiss.normalize_L2(query_vec)

#     max_search = min(100, index.ntotal)
#     search_k = req.top_k * 2
#     seen_texts = set()
#     results = []

#     while search_k <= max_search:
#         distances, indices = index.search(query_vec, search_k)
#         results.clear()
#         seen_texts.clear()

#         for i, dist in zip(indices[0], distances[0]):
#             item = search_data_list[i]
#             text = item["text"]
#             if text in seen_texts:
#                 continue
#             results.append({
#                 "text": text,
#                 "title": item.get("title", ""),
#                 "date": item.get("date", ""),
#                 "similarity": round(float(dist), 4)
#             })
#             seen_texts.add(text)
#             if len(results) >= req.top_k:
#                 return results

#         search_k *= 2

#     return results


if __name__ == "__main__":
    print("Starting FastAPI server with lifespan events...")
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
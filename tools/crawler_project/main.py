import os
import json
import numpy as np
import pandas as pd
import uvicorn
import openai
import faiss
from pathlib import Path

from fastapi import FastAPI, HTTPException, Request, Response
from pydantic import BaseModel
from dotenv import load_dotenv
from fastapi.middleware.cors import CORSMiddleware
from crawler.molit.extractor.embedding_service import embed_texts
from contextlib import asynccontextmanager
from typing import List
from pathlib import Path

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY_DEV"))
    print("OpenAI client initialized.")
except Exception as e:
    print(f"Error initializing OpenAI client: {e}")
    client = None

OPENAI_MODEL = "gpt-4o-mini"

base_dir = os.path.abspath(os.path.dirname(__file__))
cache_dir = os.path.join(base_dir, "cache")

# --- ë‰´ìŠ¤ ìš”ì•½ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ ---
summarized_news: List['SummaryOutput'] = []

# Lifespan í•¨ìˆ˜ ì •ì˜
@asynccontextmanager
async def lifespan(app: FastAPI):
    print("ğŸ”„ Loading FAISS index and reference data...")
    try:
        faiss_index = faiss.read_index(os.path.join(cache_dir, "faiss.index"))
        with open(os.path.join(cache_dir, "combined_data.json"), encoding="utf-8") as f:
            reference_data = json.load(f)

        app.state.faiss_index = faiss_index
        app.state.reference_data = reference_data
        print(f"Loaded FAISS index ({faiss_index.ntotal} vectors) and reference data.")

    except Exception as e:
        print(f" Failed to load search data: {e}")
        app.state.faiss_index = None
        app.state.reference_data = []

    # --- ë‰´ìŠ¤ ìš”ì•½ ë¡œì§ì„ lifespanì— í¬í•¨ ---
    print("ğŸ”„ Summarizing news articles...")
    try:
        base_dir = Path(__file__).resolve().parent  # summarize_api.py ìœ„ì¹˜
        file_path = base_dir / "data" / "yna_news" / "full_articles_recent.json"
        with Path(file_path).open("r", encoding="utf-8") as f:
            articles = json.load(f)

        global summarized_news
        summarized_news = [
            SummaryOutput(title=article["title"], summary=generate_summary(article["content"]), url=article["url"])
            for article in articles
        ]
        print(f"Summarized {len(summarized_news)} news articles.")
    except Exception as e:
        print(f"[ì‹œì‘ ì¤‘ ì˜¤ë¥˜] ê¸°ì‚¬ ìš”ì•½ ì‹¤íŒ¨: {e}")


    yield
    print("Server shutdown: Cleaning up resources...")

# FastAPI ì•± ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œ lifespan ì „ë‹¬
app = FastAPI(lifespan=lifespan)

# CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
origins = [
    "http://localhost",
    "http://localhost:8000",
    "http://10.0.2.2:8000",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic ëª¨ë¸ ì •ì˜ ---
class PromptRequest(BaseModel):
    prompt: str

class LLMResponse(BaseModel):
    response: str

class SearchRequest(BaseModel):
    query: str
    top_k: int = 3

class SummaryOutput(BaseModel):
    title: str
    summary: str
    url: str

# ë‰´ìŠ¤ ìš”ì•½ í•¨ìˆ˜
def generate_summary(content: str, max_tokens: int = 300) -> str:
    prompt = f"""ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì¹œêµ¬ì—ê²Œ ì„¤ëª…í•˜ë“¯ì´ ì‰½ê²Œ ìš”ì•½í•´ì¤˜.
ì–´ë ¤ìš´ ë§ì€ ì“°ì§€ ë§ê³  ì‰½ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì•Œë ¤ì¤˜. ë°˜ë§ì€ í•˜ì§€ë§ˆ. ì¹œì ˆí•˜ê³  ì–´ë ¤ìš´ ë¶€ë¶„ì—” ë¶€ì—°ì„¤ëª…ì„ ë‹¬ì•„ì„œ í•´ì¤˜\n\n{content}\n\nìš”ì•½:"""
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            temperature=0.7,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        raise RuntimeError(f"ìš”ì•½ ì‹¤íŒ¨: {e}")

@app.post("/chat", response_model=LLMResponse)
async def chat_with_llm(request: PromptRequest, http_request: Request):
    if client is None:
        raise HTTPException(status_code=500, detail="OpenAI client is not initialized.")

    try:
        print(f"Received prompt for /chat: {request.prompt}")

        # --- 1. FAISS ê²€ìƒ‰ ë¡œì§ í¬í•¨ ---
        index = app.state.faiss_index
        search_data_list = app.state.reference_data

        if index is None or not search_data_list:
            raise HTTPException(status_code=503, detail="Search index not available.")

        query_vec = embed_texts([request.prompt])[0]
        query_vec = np.array([query_vec], dtype="float32")
        faiss.normalize_L2(query_vec)

        top_k = 10 # ê²€ìƒ‰í•  ë¬¸ë‹¨ ìˆ˜
        distances, indices = index.search(query_vec, top_k)

        context_chunks = []
        for i in indices[0]:
            if i != -1:
                item = search_data_list[i]
                context_chunks.append(item["text"])

        if not context_chunks:
            raise HTTPException(status_code=404, detail="No relevant context found.")

        context_text = "\n\n".join(f"- {chunk}" for chunk in context_chunks)

        messages = [
            {
                "role": "system",
                "content": (
                            "ë‹¹ì‹ ì€ ì „ì„¸ì‚¬ê¸° ë° ì„ëŒ€ ì •ì±…ì— ëŒ€í•´ ì‹ ë¢°ì„± ìˆê³  ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤. "
            "ë‹¤ìŒì€ ì°¸ê³  ë¬¸ë‹¨ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ì´ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í•´ìš”ì²´ë¡œ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n\n"
            "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì´ëª¨í‹°ì½˜ë„ ì ì ˆíˆ ì‚¬ìš©í•˜ë©´ì„œ ë”°ëœ»í•˜ê³  ì¹œì ˆí•œ ë§íˆ¬ë¡œ í•´ì£¼ì„¸ìš”. "
            "ì „ì„¸ ì„ëŒ€ì— ëŒ€í•´ ì˜ ëª¨ë¥´ëŠ” ì‚¬íšŒì´ˆë…„ìƒì´ë¼ê³  ìƒê°í•˜ê³  ë‹µë³€ì„ í•´ì£¼ì„¸ìš”. ì–´ë ¤ìš´ ìš©ì–´ì—ì„œëŠ” ì‰½ê²Œ ì„¤ëª…ì„ ë§ë¶™ì—¬ ì£¼ì„¸ìš”.\n\n"
            f"{context_text}"
                )
            },
            {
                "role": "user",
                "content": request.prompt
            }
        ]

        # --- 3. LLM í˜¸ì¶œ ---
        completion = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=messages
        )

        llm_response_text = completion.choices[0].message.content
        return LLMResponse(response=llm_response_text)

    except openai.AuthenticationError as e:
        raise HTTPException(status_code=401, detail=f"OpenAI Authentication Error: {e}")
    except openai.APIError as e:
        raise HTTPException(status_code=500, detail=f"OpenAI API Error: {e}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An internal server error occurred: {e}")

base_dir = Path(__file__).resolve().parent.parent
bjd_code_path = base_dir.parent / 'assets' / 'config' / 'bjd_code.csv'

@app.get("/sido_code")
async def parse_sido_code():
    try:
        bjd_code_df = pd.read_csv(bjd_code_path)
        bjd_code_df.columns = [
            "code",
            "bjd_name",
            "is_abolition"
        ]

        if bjd_code_df["code"][0] == "1100000000":
            print("true")
        bjd_code_df = bjd_code_df[bjd_code_df["is_abolition"] != "íì§€"]
        sido_df = bjd_code_df[bjd_code_df["code"].apply(lambda x: str(x)[2:] == '0' * (len(str(x)) - 2))]
        sido_json = sido_df.to_json(orient="records")
        sido_json = json.dumps(json.loads(sido_json), ensure_ascii=False)
        print(sido_json)
    except Exception as e:
        print(f'ë²•ì •ë™ ì½”ë“œ íŒŒì‹± ì—ëŸ¬ {e}')
    return Response(content=sido_json, media_type="application/json")

@app.get("/sgg_code")
async def parse_bjd_sgg_code(sido_code: int):
    try:
        bjd_code_df = pd.read_csv(bjd_code_path)
        bjd_code_df.columns = [
            "code",
            "bjd_name",
            "is_abolition"
        ]

        bjd_code_df = bjd_code_df[bjd_code_df["is_abolition"] != "íì§€"]
        sgg_df = bjd_code_df[bjd_code_df["code"].apply(lambda x: str(x)[2:] != '0' * (len(str(x)) - 2) and str(x)[5:] == '0' * (len(str(x)) - 5) and str(x)[:2] == str(sido_code)[:2])]
        sgg_json = sgg_df.to_json(orient="records")
        sgg_json = json.dumps(json.loads(sgg_json), ensure_ascii=False)
    except Exception as e:
        print(f'ë²•ì •ë™ ì½”ë“œ íŒŒì‹± ì—ëŸ¬ {e}')
    return Response(content=sgg_json, media_type="application/json")

@app.get("/news", response_model=List[SummaryOutput])
async def get_summarized_news():
    if not summarized_news:
        raise HTTPException(status_code=500, detail="ìš”ì•½ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    return summarized_news

if __name__ == "__main__":
    print("Starting FastAPI server with lifespan events...")
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)